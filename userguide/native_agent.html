<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Native AI Agent (dfm agent) &#8212; DV Flow Manager  documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Flow-Spec Reference" href="../reference/index.html" />
    <link rel="prev" title="AI Agent Integration" href="agent_integration.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="native-ai-agent-dfm-agent">
<h1>Native AI Agent (dfm agent)<a class="headerlink" href="#native-ai-agent-dfm-agent" title="Link to this heading">¶</a></h1>
<p>DV Flow Manager ships a fully-embedded, native AI agent that runs entirely
within the <code class="docutils literal notranslate"><span class="pre">dfm</span></code> process — no external CLI tools required.  It is the
default when no subprocess assistant (GitHub Copilot CLI, Codex CLI) is
detected in <code class="docutils literal notranslate"><span class="pre">PATH</span></code>, and can also be selected explicitly with
<code class="docutils literal notranslate"><span class="pre">-a</span> <span class="pre">native</span></code>.</p>
<p>The native agent is powered by <a class="reference external" href="https://github.com/openai/openai-agents-python">openai-agents</a> + <a class="reference external" href="https://docs.litellm.ai/">LiteLLM</a>, which means
it supports every model provider that LiteLLM supports, including GitHub
Copilot, OpenAI, Anthropic, Google, Azure, Ollama, and any
OpenAI-compatible endpoint.</p>
<section id="quick-start">
<h2>Quick Start<a class="headerlink" href="#quick-start" title="Link to this heading">¶</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Auto-detect: uses native agent when no subprocess CLI is found</span>
dfm<span class="w"> </span>agent

<span class="c1"># Explicitly use the native agent</span>
dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native

<span class="c1"># Specify a model</span>
dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native<span class="w"> </span>-m<span class="w"> </span>openai/gpt-4o

<span class="c1"># With project context</span>
dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native<span class="w"> </span>MySkill<span class="w"> </span>MyPersona
</pre></div>
</div>
</section>
<section id="installing-the-agent-dependencies">
<h2>Installing the Agent Dependencies<a class="headerlink" href="#installing-the-agent-dependencies" title="Link to this heading">¶</a></h2>
<p>The agent dependencies are an optional extra that must be installed:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>dv-flow-mgr<span class="o">[</span>agent<span class="o">]</span>
</pre></div>
</div>
<p>Or, in a project managed by <code class="docutils literal notranslate"><span class="pre">ivpm</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add to ivpm.yaml agent dep-set, then:</span>
ivpm<span class="w"> </span>update
</pre></div>
</div>
</section>
<section id="provider-configuration">
<h2>Provider Configuration<a class="headerlink" href="#provider-configuration" title="Link to this heading">¶</a></h2>
<p>The native agent uses LiteLLM to talk to the underlying model provider.
The model is selected in this priority order:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-m</span></code> / <code class="docutils literal notranslate"><span class="pre">--model</span></code> CLI flag</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model:</span></code> key in config file (<code class="docutils literal notranslate"><span class="pre">~/.dfm/agent.yaml</span></code> or <code class="docutils literal notranslate"><span class="pre">.dfm/agent.yaml</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DFM_MODEL</span></code> environment variable</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DFM_PROVIDER</span></code> environment variable (uses <code class="docutils literal notranslate"><span class="pre">&lt;provider&gt;/gpt-4.1</span></code>)</p></li>
<li><p><strong>Auto-detected from well-known API-key environment variables</strong> (see table below)</p></li>
<li><p>Built-in default: <code class="docutils literal notranslate"><span class="pre">github_copilot/gpt-4.1</span></code></p></li>
</ol>
<p>In most cases you only need to export the right key and the agent will figure
out the provider automatically:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 35.0%" />
<col style="width: 65.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Environment variable</p></th>
<th class="head"><p>Auto-selected model</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">GITHUB_TOKEN</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">github_copilot/gpt-4.1</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">OPENAI_API_KEY</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">openai/gpt-4.1</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ANTHROPIC_API_KEY</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">anthropic/claude-3-5-sonnet-20241022</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">GEMINI_API_KEY</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">gemini/gemini-2.0-flash</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">AZURE_API_KEY</span></code> + <code class="docutils literal notranslate"><span class="pre">AZURE_API_BASE</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">azure/gpt-4o</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">OLLAMA_HOST</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ollama/llama3.2</span></code></p></td>
</tr>
</tbody>
</table>
<p>Model names follow the LiteLLM convention <code class="docutils literal notranslate"><span class="pre">&lt;provider&gt;/&lt;model-name&gt;</span></code>.</p>
<section id="github-copilot">
<h3>GitHub Copilot<a class="headerlink" href="#github-copilot" title="Link to this heading">¶</a></h3>
<p>GitHub Copilot is the default provider. It uses your existing Copilot
subscription — no separate API key is required.</p>
<p><strong>Authentication</strong> is handled via the GitHub CLI token or a <code class="docutils literal notranslate"><span class="pre">GITHUB_TOKEN</span></code>
environment variable.  The first time you use a Copilot model, LiteLLM may
trigger an OAuth device-flow prompt in the terminal.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Export a GitHub personal access token (recommended for headless use)</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">GITHUB_TOKEN</span><span class="o">=</span>ghp_...

<span class="c1"># Default Copilot model</span>
dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native

<span class="c1"># Specific Copilot model</span>
dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native<span class="w"> </span>-m<span class="w"> </span>github_copilot/gpt-4.1
dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native<span class="w"> </span>-m<span class="w"> </span>github_copilot/claude-3.7-sonnet
dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native<span class="w"> </span>-m<span class="w"> </span>github_copilot/o3-mini
</pre></div>
</div>
<p>Available Copilot model names depend on your subscription; <code class="docutils literal notranslate"><span class="pre">gpt-4.1</span></code>,
<code class="docutils literal notranslate"><span class="pre">gpt-4o</span></code>, <code class="docutils literal notranslate"><span class="pre">claude-3.7-sonnet</span></code>, and <code class="docutils literal notranslate"><span class="pre">o3-mini</span></code> are common options.</p>
<p><strong>Config file shorthand:</strong></p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># ~/.dfm/agent.yaml</span>
<span class="nt">model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">github_copilot/gpt-4.1</span>
</pre></div>
</div>
</section>
<section id="openai">
<h3>OpenAI<a class="headerlink" href="#openai" title="Link to this heading">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">OPENAI_API_KEY</span><span class="o">=</span>sk-...

<span class="c1"># GPT-4o (recommended)</span>
dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native<span class="w"> </span>-m<span class="w"> </span>openai/gpt-4o

<span class="c1"># GPT-4o-mini (faster, cheaper)</span>
dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native<span class="w"> </span>-m<span class="w"> </span>openai/gpt-4o-mini

<span class="c1"># o1 reasoning model</span>
dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native<span class="w"> </span>-m<span class="w"> </span>openai/o1
</pre></div>
</div>
<p><strong>Config file:</strong></p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># ~/.dfm/agent.yaml</span>
<span class="nt">model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">openai/gpt-4o</span>
</pre></div>
</div>
</section>
<section id="anthropic-claude">
<h3>Anthropic Claude<a class="headerlink" href="#anthropic-claude" title="Link to this heading">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">ANTHROPIC_API_KEY</span><span class="o">=</span>sk-ant-...

dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native<span class="w"> </span>-m<span class="w"> </span>anthropic/claude-3-5-sonnet-20241022
dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native<span class="w"> </span>-m<span class="w"> </span>anthropic/claude-3-opus-20240229
</pre></div>
</div>
<p><strong>Config file:</strong></p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">anthropic/claude-3-5-sonnet-20241022</span>
</pre></div>
</div>
</section>
<section id="azure-openai">
<h3>Azure OpenAI<a class="headerlink" href="#azure-openai" title="Link to this heading">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">AZURE_API_KEY</span><span class="o">=</span>...
<span class="nb">export</span><span class="w"> </span><span class="nv">AZURE_API_BASE</span><span class="o">=</span>https://your-resource.openai.azure.com/
<span class="nb">export</span><span class="w"> </span><span class="nv">AZURE_API_VERSION</span><span class="o">=</span><span class="m">2024</span>-02-01

dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native<span class="w"> </span>-m<span class="w"> </span>azure/your-deployment-name
</pre></div>
</div>
<p><strong>Config file:</strong></p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">azure/your-deployment-name</span>
</pre></div>
</div>
</section>
<section id="custom-http-headers-and-api-gateway-authentication">
<h3>Custom HTTP Headers and API Gateway Authentication<a class="headerlink" href="#custom-http-headers-and-api-gateway-authentication" title="Link to this heading">¶</a></h3>
<p>Some organisations route model requests through a proxy or API gateway that
requires an auth token or subscription key in a custom HTTP header.  Use
<code class="docutils literal notranslate"><span class="pre">model_settings.headers</span></code> in the config file:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># .dfm/agent.yaml</span>
<span class="nt">model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">openai/gpt-4o</span>
<span class="nt">model_settings</span><span class="p">:</span>
<span class="w">  </span><span class="nt">api_base</span><span class="p">:</span><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">https://llm-proxy.example.com</span>
<span class="w">  </span><span class="nt">api_key</span><span class="p">:</span><span class="w">     </span><span class="s">&quot;${{</span><span class="nv"> </span><span class="s">env.LLM_API_KEY</span><span class="nv"> </span><span class="s">}}&quot;</span>
<span class="w">  </span><span class="nt">api_version</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;2024-06-01&quot;</span>
<span class="w">  </span><span class="nt">ssl_verify</span><span class="p">:</span><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">  </span><span class="nt">headers</span><span class="p">:</span>
<span class="w">    </span><span class="nt">X-Auth-Token</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;${{</span><span class="nv"> </span><span class="s">env.LLM_AUTH_TOKEN</span><span class="nv"> </span><span class="s">}}&quot;</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">${{</span> <span class="pre">env.VAR</span> <span class="pre">}}</span></code> syntax expands the named environment variable at
config-load time, so the key never has to be stored in plain text:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LLM_AUTH_TOKEN</span><span class="o">=</span>my-secret-token
dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native
</pre></div>
</div>
<p>All entries under <code class="docutils literal notranslate"><span class="pre">model_settings</span></code> are passed directly to the underlying
LiteLLM <code class="docutils literal notranslate"><span class="pre">acompletion()</span></code> call:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 25.0%" />
<col style="width: 75.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Key</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">api_base</span></code></p></td>
<td><p>Override the endpoint URL</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">api_key</span></code></p></td>
<td><p>Override the API key (can use <code class="docutils literal notranslate"><span class="pre">${{</span> <span class="pre">env.VAR</span> <span class="pre">}}</span></code>)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">api_version</span></code></p></td>
<td><p>API version string (required for Azure)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ssl_verify</span></code></p></td>
<td><p>Set to <code class="docutils literal notranslate"><span class="pre">false</span></code> to disable TLS certificate verification</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">headers</span></code></p></td>
<td><p>Dict of custom HTTP headers added to every request</p></td>
</tr>
</tbody>
</table>
</section>
<section id="google-gemini">
<h3>Google Gemini<a class="headerlink" href="#google-gemini" title="Link to this heading">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">GEMINI_API_KEY</span><span class="o">=</span>...

dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native<span class="w"> </span>-m<span class="w"> </span>gemini/gemini-1.5-pro
dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native<span class="w"> </span>-m<span class="w"> </span>gemini/gemini-1.5-flash
</pre></div>
</div>
</section>
<section id="openai-compatible-endpoints-vllm-lm-studio-etc">
<h3>OpenAI-Compatible Endpoints (vLLM, LM Studio, etc.)<a class="headerlink" href="#openai-compatible-endpoints-vllm-lm-studio-etc" title="Link to this heading">¶</a></h3>
<p>Any server that implements the OpenAI chat-completions API can be used.
Set <code class="docutils literal notranslate"><span class="pre">OPENAI_API_BASE</span></code> (or <code class="docutils literal notranslate"><span class="pre">OPENAI_BASE_URL</span></code>) to point at your server:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">OPENAI_API_BASE</span><span class="o">=</span>http://my-server:8000/v1
<span class="nb">export</span><span class="w"> </span><span class="nv">OPENAI_API_KEY</span><span class="o">=</span>dummy<span class="w">          </span><span class="c1"># required by LiteLLM even if unused</span>

dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native<span class="w"> </span>-m<span class="w"> </span>openai/my-deployed-model
</pre></div>
</div>
</section>
<section id="ollama-local-models">
<h3>Ollama (Local Models)<a class="headerlink" href="#ollama-local-models" title="Link to this heading">¶</a></h3>
<p><a class="reference external" href="https://ollama.com">Ollama</a> runs open-weight models locally.  Install Ollama and pull a model,
then point LiteLLM at it:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Start Ollama (it runs on http://localhost:11434 by default)</span>
ollama<span class="w"> </span>pull<span class="w"> </span>llama3.2
ollama<span class="w"> </span>pull<span class="w"> </span>qwen2.5-coder:7b

<span class="c1"># Run via LiteLLM&#39;s ollama provider</span>
dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native<span class="w"> </span>-m<span class="w"> </span>ollama/llama3.2
dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native<span class="w"> </span>-m<span class="w"> </span>ollama/qwen2.5-coder:7b

<span class="c1"># Or set DFM_MODEL to avoid typing it every time</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DFM_MODEL</span><span class="o">=</span>ollama/qwen2.5-coder:7b
dfm<span class="w"> </span>agent
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Smaller models (7 B parameters and below) may struggle with complex
multi-tool workflows.  <code class="docutils literal notranslate"><span class="pre">qwen2.5-coder:14b</span></code>, <code class="docutils literal notranslate"><span class="pre">llama3.1:8b</span></code>, or
<code class="docutils literal notranslate"><span class="pre">mistral-nemo</span></code> are reasonable minimum sizes for productive sessions.</p>
</div>
<p>Ollama on a remote host:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">OLLAMA_API_BASE</span><span class="o">=</span>http://gpu-server:11434
dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native<span class="w"> </span>-m<span class="w"> </span>ollama/llama3.2
</pre></div>
</div>
</section>
</section>
<section id="environment-variables-summary">
<h2>Environment Variables Summary<a class="headerlink" href="#environment-variables-summary" title="Link to this heading">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 30.0%" />
<col style="width: 70.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Variable</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">DFM_MODEL</span></code></p></td>
<td><p>Full LiteLLM model name, e.g. <code class="docutils literal notranslate"><span class="pre">openai/gpt-4o</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">DFM_PROVIDER</span></code></p></td>
<td><p>Provider prefix only; model defaults to <code class="docutils literal notranslate"><span class="pre">&lt;provider&gt;/gpt-4.1</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">GITHUB_TOKEN</span></code></p></td>
<td><p>GitHub personal-access-token; <strong>auto-selects</strong> <code class="docutils literal notranslate"><span class="pre">github_copilot/gpt-4.1</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">OPENAI_API_KEY</span></code></p></td>
<td><p>OpenAI (or OpenAI-compatible) API key; <strong>auto-selects</strong> <code class="docutils literal notranslate"><span class="pre">openai/gpt-4.1</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">OPENAI_API_BASE</span></code></p></td>
<td><p>Override base URL for OpenAI-compatible servers</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ANTHROPIC_API_KEY</span></code></p></td>
<td><p>Anthropic API key; <strong>auto-selects</strong> <code class="docutils literal notranslate"><span class="pre">anthropic/claude-3-5-sonnet-20241022</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">AZURE_API_KEY</span></code></p></td>
<td><p>Azure OpenAI key; combined with <code class="docutils literal notranslate"><span class="pre">AZURE_API_BASE</span></code> <strong>auto-selects</strong> <code class="docutils literal notranslate"><span class="pre">azure/gpt-4o</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">AZURE_API_BASE</span></code></p></td>
<td><p>Azure OpenAI endpoint URL</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">AZURE_API_VERSION</span></code></p></td>
<td><p>Azure OpenAI API version string</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">GEMINI_API_KEY</span></code></p></td>
<td><p>Google Gemini API key; <strong>auto-selects</strong> <code class="docutils literal notranslate"><span class="pre">gemini/gemini-2.0-flash</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">OLLAMA_HOST</span></code></p></td>
<td><p>Ollama server URL; <strong>auto-selects</strong> <code class="docutils literal notranslate"><span class="pre">ollama/llama3.2</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">OLLAMA_API_BASE</span></code></p></td>
<td><p>Alternative Ollama server URL (used by LiteLLM directly)</p></td>
</tr>
</tbody>
</table>
</section>
<section id="config-file">
<h2>Config File<a class="headerlink" href="#config-file" title="Link to this heading">¶</a></h2>
<p>Create <code class="docutils literal notranslate"><span class="pre">~/.dfm/agent.yaml</span></code> (user-wide) and/or <code class="docutils literal notranslate"><span class="pre">.dfm/agent.yaml</span></code>
(project-local, takes precedence) to set persistent defaults:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># ~/.dfm/agent.yaml  or  .dfm/agent.yaml</span>
<span class="nt">model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">github_copilot/gpt-4.1</span>

<span class="c1"># Tool approval mode: never | auto | write</span>
<span class="c1">#   never  – run all tools automatically (default)</span>
<span class="c1">#   auto   – prompt before shell/write tools</span>
<span class="c1">#   write  – alias for auto</span>
<span class="nt">approval_mode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">never</span>

<span class="c1"># Enable agent tracing (writes JSONL to trace_dir)</span>
<span class="nt">trace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">trace_dir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">~/.dfm/traces/</span>

<span class="c1"># Extra text appended to the generated system prompt</span>
<span class="nt">system_prompt_extra</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">    </span><span class="no">Always prefer minimal, targeted code changes.</span>

<span class="c1"># LiteLLM model-level settings (all optional)</span>
<span class="nt">model_settings</span><span class="p">:</span>
<span class="w">  </span><span class="nt">api_base</span><span class="p">:</span><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">https://llm-proxy.example.com</span><span class="w">  </span><span class="c1"># override endpoint</span>
<span class="w">  </span><span class="nt">api_key</span><span class="p">:</span><span class="w">     </span><span class="s">&quot;${{</span><span class="nv"> </span><span class="s">env.LLM_API_KEY</span><span class="nv"> </span><span class="s">}}&quot;</span><span class="w">       </span><span class="c1"># or plain string</span>
<span class="w">  </span><span class="nt">api_version</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;2024-06-01&quot;</span><span class="w">                   </span><span class="c1"># e.g. Azure API version</span>
<span class="w">  </span><span class="nt">ssl_verify</span><span class="p">:</span><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w">                           </span><span class="c1"># disable TLS verification</span>
<span class="w">  </span><span class="nt">headers</span><span class="p">:</span><span class="w">                                    </span><span class="c1"># custom HTTP request headers</span>
<span class="w">    </span><span class="nt">X-Auth-Token</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;${{</span><span class="nv"> </span><span class="s">env.LLM_AUTH_TOKEN</span><span class="nv"> </span><span class="s">}}&quot;</span>
<span class="w">    </span><span class="nt">X-Custom-Header</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">some-value</span>

<span class="c1"># Additional MCP servers to start (advanced)</span>
<span class="nt">mcp_servers</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">my-tool</span>
<span class="w">    </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">uvx</span>
<span class="w">    </span><span class="nt">args</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">mcp-my-tool</span><span class="p p-Indicator">]</span>
</pre></div>
</div>
<p>CLI flags always override config-file values.</p>
<section id="environment-variable-references">
<h3>Environment Variable References<a class="headerlink" href="#environment-variable-references" title="Link to this heading">¶</a></h3>
<p>Any string value in the config file can reference an environment variable
using <code class="docutils literal notranslate"><span class="pre">${{</span> <span class="pre">env.VAR_NAME</span> <span class="pre">}}</span></code> syntax.  This is evaluated at load time:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">model_settings</span><span class="p">:</span>
<span class="w">  </span><span class="nt">api_key</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;${{</span><span class="nv"> </span><span class="s">env.OPENAI_API_KEY</span><span class="nv"> </span><span class="s">}}&quot;</span>
<span class="w">  </span><span class="nt">headers</span><span class="p">:</span>
<span class="w">    </span><span class="nt">X-Auth-Token</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;${{</span><span class="nv"> </span><span class="s">env.LLM_AUTH_TOKEN</span><span class="nv"> </span><span class="s">}}&quot;</span>
<span class="w">    </span><span class="nt">Authorization</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Bearer</span><span class="nv"> </span><span class="s">${{</span><span class="nv"> </span><span class="s">env.MY_BEARER_TOKEN</span><span class="nv"> </span><span class="s">}}&quot;</span>
</pre></div>
</div>
<p>If the referenced variable is not set, the value expands to an empty string
and a warning is logged.</p>
</section>
</section>
<section id="tui-interaction">
<h2>TUI Interaction<a class="headerlink" href="#tui-interaction" title="Link to this heading">¶</a></h2>
<p>The native agent presents a Rich + prompt_toolkit terminal UI with streaming
Markdown output and colour-coded tool-call panels.</p>
<section id="slash-commands">
<h3>Slash Commands<a class="headerlink" href="#slash-commands" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<colgroup>
<col style="width: 30.0%" />
<col style="width: 70.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Command</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">/help</span></code></p></td>
<td><p>Show all slash commands</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">/model</span></code></p></td>
<td><p>Display the active model name</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">/tools</span></code></p></td>
<td><p>List all registered tools</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">/skills</span></code></p></td>
<td><p>List skills and personas defined in the project</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">/personas</span></code></p></td>
<td><p>Alias for <code class="docutils literal notranslate"><span class="pre">/skills</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">/skill</span> <span class="pre">add</span> <span class="pre">&lt;Name&gt;</span></code></p></td>
<td><p>Hot-load a skill into the current session</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">/persona</span> <span class="pre">add</span> <span class="pre">&lt;Name&gt;</span></code></p></td>
<td><p>Hot-load a persona into the current session</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">/cost</span></code></p></td>
<td><p>Show cumulative token usage for the session</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">/approval</span> <span class="pre">[mode]</span></code></p></td>
<td><p>Show or set tool approval mode (<code class="docutils literal notranslate"><span class="pre">never</span></code> / <code class="docutils literal notranslate"><span class="pre">auto</span></code> / <code class="docutils literal notranslate"><span class="pre">write</span></code>)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">/clear</span></code></p></td>
<td><p>Clear conversation history</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">/exit</span></code>, <code class="docutils literal notranslate"><span class="pre">/quit</span></code></p></td>
<td><p>Exit the agent</p></td>
</tr>
</tbody>
</table>
</section>
<section id="keyboard-shortcuts">
<h3>Keyboard Shortcuts<a class="headerlink" href="#keyboard-shortcuts" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>Ctrl+D</strong> — exit immediately (same as <code class="docutils literal notranslate"><span class="pre">/exit</span></code>)</p></li>
<li><p><strong>Ctrl+C</strong> — cancel the current response; press again within 1 second to exit</p></li>
<li><p><strong>Up / Down</strong> — navigate input history</p></li>
</ul>
</section>
<section id="approval-mode">
<h3>Approval Mode<a class="headerlink" href="#approval-mode" title="Link to this heading">¶</a></h3>
<p>By default (<code class="docutils literal notranslate"><span class="pre">never</span></code>), all tool calls execute automatically.  Set
<code class="docutils literal notranslate"><span class="pre">approval_mode:</span> <span class="pre">auto</span></code> (or use <code class="docutils literal notranslate"><span class="pre">--approval-mode</span> <span class="pre">auto</span></code>) to be prompted
before any <code class="docutils literal notranslate"><span class="pre">shell_exec</span></code>, <code class="docutils literal notranslate"><span class="pre">write_file</span></code>, or <code class="docutils literal notranslate"><span class="pre">apply_patch</span></code> call:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>dfm<span class="w"> </span>agent<span class="w"> </span>--approval-mode<span class="w"> </span>auto
</pre></div>
</div>
<p>You can also change the mode mid-session:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&gt; /approval auto
Approval mode set to: auto

&gt; /approval never
Approval mode set to: never
</pre></div>
</div>
</section>
<section id="tracing">
<h3>Tracing<a class="headerlink" href="#tracing" title="Link to this heading">¶</a></h3>
<p>Enable detailed span tracing for debugging or auditing:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>dfm<span class="w"> </span>agent<span class="w"> </span>--trace
</pre></div>
</div>
<p>Traces are written as JSONL to <code class="docutils literal notranslate"><span class="pre">~/.dfm/traces/trace_&lt;timestamp&gt;.jsonl</span></code>.</p>
</section>
</section>
<section id="available-tools">
<h2>Available Tools<a class="headerlink" href="#available-tools" title="Link to this heading">¶</a></h2>
<p>The native agent has access to two sets of built-in tools.</p>
<section id="dfm-tools-blue-panels">
<h3>DFM Tools (blue panels)<a class="headerlink" href="#dfm-tools-blue-panels" title="Link to this heading">¶</a></h3>
<p>These tools give the agent direct access to DV Flow Manager:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 25.0%" />
<col style="width: 75.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Tool</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">dfm_show_tasks</span></code></p></td>
<td><p>List all tasks in the loaded project</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">dfm_show_task</span></code></p></td>
<td><p>Get detailed information about a specific task</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">dfm_show_packages</span></code></p></td>
<td><p>List imported packages</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">dfm_show_types</span></code></p></td>
<td><p>List available task types</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">dfm_show_skills</span></code></p></td>
<td><p>List skills and personas</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">dfm_context</span></code></p></td>
<td><p>Return complete project context as JSON</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">dfm_validate</span></code></p></td>
<td><p>Validate the current flow definition</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">dfm_run_tasks</span></code></p></td>
<td><p>Execute one or more tasks</p></td>
</tr>
</tbody>
</table>
</section>
<section id="coding-tools-yellow-green-panels">
<h3>Coding Tools (yellow / green panels)<a class="headerlink" href="#coding-tools-yellow-green-panels" title="Link to this heading">¶</a></h3>
<p>These general-purpose tools let the agent read and modify files:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 25.0%" />
<col style="width: 75.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Tool</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">shell_exec</span></code></p></td>
<td><p>Run a shell command <em>(yellow — requires approval in auto mode)</em></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">write_file</span></code></p></td>
<td><p>Write content to a file <em>(yellow — requires approval in auto mode)</em></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">apply_patch</span></code></p></td>
<td><p>Apply a unified diff <em>(yellow — requires approval in auto mode)</em></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">read_file</span></code></p></td>
<td><p>Read a file <em>(green)</em></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">list_directory</span></code></p></td>
<td><p>List directory contents <em>(green)</em></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">grep_search</span></code></p></td>
<td><p>Search file contents with a regex <em>(green)</em></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="subprocess-agents-legacy">
<h2>Subprocess Agents (Legacy)<a class="headerlink" href="#subprocess-agents-legacy" title="Link to this heading">¶</a></h2>
<p>The original subprocess-based agents (GitHub Copilot CLI, OpenAI Codex CLI)
are still supported via <code class="docutils literal notranslate"><span class="pre">--assistant</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># GitHub Copilot CLI (must be installed separately)</span>
dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>copilot

<span class="c1"># OpenAI Codex CLI (must be installed separately)</span>
dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>codex
</pre></div>
</div>
<p>These agents communicate with DFM through a JSON result-file protocol and do
not have the streaming TUI or direct tool access.  They remain available for
environments where the native agent dependencies cannot be installed.</p>
</section>
<section id="troubleshooting">
<h2>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Link to this heading">¶</a></h2>
<section id="no-module-named-agents">
<h3><code class="docutils literal notranslate"><span class="pre">No</span> <span class="pre">module</span> <span class="pre">named</span> <span class="pre">'agents'</span></code><a class="headerlink" href="#no-module-named-agents" title="Link to this heading">¶</a></h3>
<p>The agent optional dependencies are not installed.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>dv-flow-mgr<span class="o">[</span>agent<span class="o">]</span>
</pre></div>
</div>
</section>
<section id="authentication-errors-401-unauthorized">
<h3>Authentication errors / <code class="docutils literal notranslate"><span class="pre">401</span> <span class="pre">Unauthorized</span></code><a class="headerlink" href="#authentication-errors-401-unauthorized" title="Link to this heading">¶</a></h3>
<p>Verify your API key is exported in the current shell:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">echo</span><span class="w"> </span><span class="nv">$OPENAI_API_KEY</span><span class="w">      </span><span class="c1"># should print your key</span>
<span class="nb">echo</span><span class="w"> </span><span class="nv">$GITHUB_TOKEN</span><span class="w">        </span><span class="c1"># for Copilot</span>
</pre></div>
</div>
<p>For GitHub Copilot, re-run <code class="docutils literal notranslate"><span class="pre">gh</span> <span class="pre">auth</span> <span class="pre">login</span></code> if the token has expired.</p>
</section>
<section id="rate-limit-reached">
<h3><code class="docutils literal notranslate"><span class="pre">Rate</span> <span class="pre">limit</span> <span class="pre">reached</span></code><a class="headerlink" href="#rate-limit-reached" title="Link to this heading">¶</a></h3>
<p>The agent will display a user-friendly message and the <code class="docutils literal notranslate"><span class="pre">run_once</span></code> path
automatically retries with exponential backoff.  In the TUI, wait a moment
and re-submit your message.</p>
</section>
<section id="copilot-oauth-prompt-in-headless-environments">
<h3>Copilot OAuth prompt in headless environments<a class="headerlink" href="#copilot-oauth-prompt-in-headless-environments" title="Link to this heading">¶</a></h3>
<p>Set <code class="docutils literal notranslate"><span class="pre">GITHUB_TOKEN</span></code> explicitly to avoid the interactive OAuth device flow:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">GITHUB_TOKEN</span><span class="o">=</span><span class="k">$(</span>gh<span class="w"> </span>auth<span class="w"> </span>token<span class="k">)</span>
dfm<span class="w"> </span>agent<span class="w"> </span>-a<span class="w"> </span>native
</pre></div>
</div>
</section>
<section id="ollama-model-not-responding">
<h3>Ollama model not responding<a class="headerlink" href="#ollama-model-not-responding" title="Link to this heading">¶</a></h3>
<p>Ensure the Ollama server is running:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>serve<span class="w"> </span><span class="p">&amp;</span>
ollama<span class="w"> </span>list<span class="w">           </span><span class="c1"># confirm the model is pulled</span>
</pre></div>
</div>
<p>Small models may time out on complex prompts.  Try a larger model or simplify
the query.</p>
</section>
</section>
<section id="see-also">
<h2>See Also<a class="headerlink" href="#see-also" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="agent_integration.html"><span class="doc">AI Agent Integration</span></a> — Defining skills, personas, tools, and references</p></li>
<li><p><a class="reference internal" href="llm_integration.html"><span class="doc">LLM Integration</span></a> — General LLM integration guide</p></li>
<li><p><a class="reference internal" href="../cmdref.html"><span class="doc">Command reference</span></a> — Full command reference</p></li>
<li><p><a class="reference internal" href="../llms.html"><span class="doc">LLM Agent Integration</span></a> — LLM call interface (server mode, parameter overrides)</p></li>
</ul>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">DV Flow Manager</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">User Guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="fundamentals.html">Key Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="packages.html">Packages</a></li>
<li class="toctree-l2"><a class="reference internal" href="tasks_using.html">Using Tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="dataflow.html">Dataflow &amp; Produces</a></li>
<li class="toctree-l2"><a class="reference internal" href="visibility.html">Task Visibility</a></li>
<li class="toctree-l2"><a class="reference internal" href="tasks_developing.html">Developing Tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="incremental.html">Incremental Builds</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration.html">Parameters and Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="expressions.html">Expressions</a></li>
<li class="toctree-l2"><a class="reference internal" href="filters.html">Filters</a></li>
<li class="toctree-l2"><a class="reference internal" href="advanced_features.html">Advanced Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="stdlib.html">Standard Library</a></li>
<li class="toctree-l2"><a class="reference internal" href="llm_integration.html">LLM Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="agent_integration.html">AI Agent Integration</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Native AI Agent (dfm agent)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#dataflow">Dataflow</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../reference/index.html">Flow-Spec Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/types_api.html">Type System API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cmdref.html">Command reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llms.html">LLM Agent Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytask_api.html">Python Task API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../task_runners.html">Task Runners</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="index.html">User Guide</a><ul>
      <li>Previous: <a href="agent_integration.html" title="previous chapter">AI Agent Integration</a></li>
      <li>Next: <a href="../reference/index.html" title="next chapter">Flow-Spec Reference</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2023-2025, Matthew Ballance.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/userguide/native_agent.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>